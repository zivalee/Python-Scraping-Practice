from urllib.request import urlopenfrom urllib.error import HTTPErrorfrom bs4 import BeautifulSoupimport reimport datetimeimport randomrandom.seed(datetime.datetime.now())def getlinks(url):    try:        html = urlopen('https://en.wikipedia.org' + url)    except HTTPError as e:        return None    try:        soup = BeautifulSoup(html, 'lxml')        return soup.find('div', {'id': 'bodyContent'}).findAll('a', href = re.compile('^(/wiki/)'))    except AttributeError as e:        return Nonelinks = getlinks('/wiki/ICloud_leaks_of_celebrity_photos')while len(links) > 0:    new = links[random.randint(0, len(links)-1)].attrs['href']    print(new)    links = getlinks(new)